{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "import sys\n",
    "\n",
    "task = \"bios\"\n",
    "split = \"dev\"\n",
    "\n",
    "model_path = f\"../experiments/models/{task}/roberta-large\"\n",
    "\n",
    "dev_encodings_normal = np.load(model_path + f\"/encodings/predicted_{task}_{split}_encoded_representations.npy\")\n",
    "dev_preds_normal = np.load(model_path + f\"/encodings/predicted_{task}_{split}_predictions.npy\")\n",
    "enc_size = dev_encodings_normal.shape[1]\n",
    "\n",
    "w = np.load(model_path + \"/w.npy\")  # shape: (num_classes, enc_size)\n",
    "b = np.load(model_path + \"/b.npy\")  # shape: (num_classes)\n",
    "with open(model_path + \"/label2index.json\", \"r\") as f:\n",
    "    label2index = json.load(f)\n",
    "    index2label = {label2index[k]: k for k in label2index}\n",
    "    \n",
    "index2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dev_path = f\"../data/{task}/{split}.jsonl\"\n",
    "\n",
    "with open(cached_path(dev_path)) as f:\n",
    "    dev_data = [json.loads(line) for line in f if line.strip() if line.strip()]\n",
    "# print([d['gold_label'] for d in dev_data])\n",
    "dev_labels = np.array([label2index[d['label']] for d in dev_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "\n",
    "orig_encodings = dev_encodings_normal\n",
    "orig_labels = dev_labels\n",
    "orig_predictions = dev_preds_normal\n",
    "\n",
    "classifier_w = w\n",
    "classifier_b = b\n",
    "\n",
    "def dkl(p,q):\n",
    "    p = p / p.sum(axis=1).reshape(-1, 1)\n",
    "    q = q / q.sum(axis=1).reshape(-1, 1)\n",
    "    return entropy(p,q, axis=1) + entropy(q,p, axis=1)\n",
    "\n",
    "for fact in index2label:\n",
    "    drop = []\n",
    "    print(f\"\\t\\t\\t\\t\\tfact: {index2label[fact]}\")\n",
    "    for foil in index2label:\n",
    "        if fact == foil:\n",
    "            drop.append(-1000)\n",
    "            continue\n",
    "            \n",
    "        contrastive_examples = np.any([orig_predictions == fact], axis=0)\n",
    "    \n",
    "        encodings = orig_encodings[contrastive_examples]\n",
    "        predictions = orig_predictions[contrastive_examples]\n",
    "        \n",
    "        if len(encodings) == 0:\n",
    "            drop.append(0)\n",
    "            continue\n",
    "\n",
    "        u = classifier_w[fact] - classifier_w[foil]\n",
    "        contrastive_rowspace_projection = np.outer(u, u) / u.dot(u)\n",
    "    \n",
    "        encodings_in_contrastive_rowspace = encodings @ contrastive_rowspace_projection #@ directional_nullspace\n",
    "\n",
    "        assert (predictions == np.argmax(encodings @ classifier_w.T + classifier_b, axis=1)).all()\n",
    "\n",
    "        predictions_in_contrastive_space = np.argmax(encodings_in_contrastive_rowspace @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "        prediction_probabilities = softmax(encodings @ classifier_w.T + classifier_b, axis=1)\n",
    "        prediction_probabilities_contrastive = softmax(encodings_in_contrastive_rowspace @ classifier_w.T + classifier_b, axis=1)\n",
    "\n",
    "        dist10 = dkl(prediction_probabilities_contrastive,\n",
    "                    prediction_probabilities).mean()\n",
    "\n",
    "        drop.append(dist10)\n",
    "\n",
    "    tab = []\n",
    "    print(fact)\n",
    "    for c in range(len(index2label)):\n",
    "        if c == fact:\n",
    "            continue\n",
    "        tab.append({\n",
    "            'foil idx': c,\n",
    "            'foil': index2label[c],\n",
    "            'Dkl': drop[c],\n",
    "        })\n",
    "\n",
    "    tab = sorted(tab, key=lambda v: -v['Dkl'])\n",
    "\n",
    "    display(pd.DataFrame(tab))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
